{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab9b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9cd9c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 17:56:46.478041: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748530606.495295   28321 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748530606.500965   28321 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748530606.514869   28321 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748530606.514891   28321 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748530606.514892   28321 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748530606.514894   28321 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-29 17:56:46.518911: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c8824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0fbad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./train_data_without_dreamt.csv')\n",
    "val_df = pd.read_csv('./validation_data_without_dreamt.csv')\n",
    "test_df = pd.read_csv('./test_data_without_dreamt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0bf8434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD SLIDING WINDOW\n",
    "# df - dataframe used\n",
    "# window_size - size of the sliding window, by default 11s if not mentioned otherwise\n",
    "# step_size - starting point for the current window given the previous, by default 5\n",
    "# feature_cols - features to be used in the sliding window\n",
    "def create_windows(dataset, window_size=11, step_size=5, feature_cols=['ACC_X', 'ACC_Y', 'ACC_Z']):\n",
    "    X = []\n",
    "    y = []\n",
    "    window = []\n",
    "\n",
    "    for person_id in dataset['PERSON_ID'].unique():\n",
    "        person_data = dataset[dataset['PERSON_ID'] == person_id]\n",
    "        feature_values = person_data[feature_cols].values\n",
    "        activity = person_data['ACTIVITY']\n",
    "\n",
    "        max_window_end = len(person_data)\n",
    "\n",
    "        for i in range(0, max_window_end - window_size, step_size):\n",
    "            window = feature_values[i:i+window_size]\n",
    "            window_label = activity[i:i+window_size].mode(dropna=False).iloc[0]\n",
    "\n",
    "            # Ensure the window is of the correct size\n",
    "            if len(window) != window_size:\n",
    "                continue  # Skip this window if it's the wrong shape\n",
    "\n",
    "            X.append(window)\n",
    "            y.append(window_label)\n",
    "\n",
    "    print(len(X))\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72f0a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 60\n",
    "step_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92f41f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442815\n",
      "124699\n",
      "65346\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = create_windows(train_df, window_size, step_size)\n",
    "X_val, y_val = create_windows(val_df, window_size, step_size)\n",
    "X_test, y_test = create_windows(test_df, window_size, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d7e6781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_window_distribution(y_labels):\n",
    "    class_counts = Counter(y_labels)\n",
    "    sorted_counts = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"Window count per class (descending):\")\n",
    "    for label, count in sorted_counts:\n",
    "        print(f\"{label:20} {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f124705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window count per class (descending):\n",
      "sleep                162381\n",
      "sitting              161927\n",
      "household-chores     28316\n",
      "walking              27028\n",
      "vehicle              16050\n",
      "mixed-activity       15950\n",
      "standing             14603\n",
      "bicycling            4730\n",
      "manual-work          3544\n",
      "sports               1793\n",
      "writing              453\n",
      "jogging              449\n",
      "drinking             448\n",
      "eating pasta         443\n",
      "dribbling (basket ball) 441\n",
      "eating chips         437\n",
      "eating sandwich      436\n",
      "brushing teeth       434\n",
      "kicking (soccer ball) 433\n",
      "clapping             433\n",
      "eating soup          431\n",
      "playing catch (tennis ball) 431\n",
      "typing               430\n",
      "stairs               424\n",
      "folding clothes      370\n"
     ]
    }
   ],
   "source": [
    "print_window_distribution(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "340250f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = np.full(len(X_train), 1) # initialize weights array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec44103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_upweight_majority_class(X_train, y_train, downsample_factor, majority_class, sample_weights):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    majority_class_indices = np.where(y_train == majority_class)[0]\n",
    "\n",
    "    X_majority_class = X_train[majority_class_indices]\n",
    "    y_majority_class = y_train[majority_class_indices]\n",
    "\n",
    "    print(f\"total number of rows on X for majority class {majority_class}: {len(X_majority_class)}\")\n",
    "    print(f\"total number of rows on y for majority class {majority_class}: {len(y_majority_class)}\")\n",
    "\n",
    "    number_of_majority_samples = len(X_majority_class)\n",
    "    number_of_samples_to_extract = number_of_majority_samples // downsample_factor\n",
    "\n",
    "    random_chosen_indices = np.random.choice(number_of_majority_samples, number_of_samples_to_extract, replace=False)\n",
    "\n",
    "    # downsampled_X = X_majority_class[random_chosen_indices]\n",
    "    # downsampled_y = y_majority_class[random_chosen_indices]\n",
    "\n",
    "    new_X_train = []\n",
    "    new_y_train = []\n",
    "    new_sample_weights = []\n",
    "\n",
    "    selected_majority_indices = majority_class_indices[random_chosen_indices]\n",
    "\n",
    "    for index in range(0, len(X_train)):\n",
    "        if index in selected_majority_indices:\n",
    "            new_X_train.append(X_train[index])\n",
    "            new_sample_weights.append(sample_weights[index] * downsample_factor)\n",
    "            new_y_train.append(y_train[index])\n",
    "        elif index in majority_class_indices:\n",
    "            continue\n",
    "        else:\n",
    "            new_X_train.append(X_train[index])\n",
    "            new_sample_weights.append(sample_weights[index])\n",
    "            new_y_train.append(y_train[index])\n",
    "    \n",
    "    return np.array(new_X_train), np.array(new_y_train), np.array(new_sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35088a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee557072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE DATA FOR THIS MODEL\n",
    "scaler = StandardScaler()\n",
    "\n",
    "n_samples = X_train.shape[0]\n",
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f5ea931",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flat = X_train.reshape(-1, X_train.shape[-1])  # Flatten each window into a 1D array\n",
    "X_val_flat = X_val.reshape(-1, X_val.shape[-1])\n",
    "X_test_flat = X_test.reshape(-1, X_test.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "393c6c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train_flat).reshape(n_samples, n_timesteps, n_features)\n",
    "X_val_scaled = scaler.transform(X_val_flat).reshape(X_val.shape[0], n_timesteps, n_features)\n",
    "X_test_scaled = scaler.transform(X_test_flat).reshape(X_test.shape[0], n_timesteps, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2d23fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODE LABELS\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_val_enc = le.transform(y_val)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_cat = to_categorical(y_train_enc)\n",
    "y_val_cat = to_categorical(y_val_enc)\n",
    "y_test_cat = to_categorical(y_test_enc)\n",
    "\n",
    "num_classes = y_train_cat.shape[1]  # Number of unique classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0989453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    # Monitor validation loss\n",
    "    patience=5,            # Stop after 5 epochs of no improvement in val_loss\n",
    "    restore_best_weights=True, # Restore weights from the epoch with the best val_loss\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2425a8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 17:57:42.693506: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2025-05-29 17:57:42.693534: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n",
      "2025-05-29 17:57:42.693538: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: frenovo\n",
      "2025-05-29 17:57:42.693541: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: frenovo\n",
      "2025-05-29 17:57:42.693622: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 570.133.7\n",
      "2025-05-29 17:57:42.693639: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 570.133.7\n",
      "2025-05-29 17:57:42.693641: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 570.133.7\n",
      "/home/franci/licenta/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "# Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(128, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]), return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffb137da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a6b56b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 44ms/step - accuracy: 0.4566 - loss: 1.5032 - val_accuracy: 0.5912 - val_loss: 1.2151\n",
      "Epoch 2/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 44ms/step - accuracy: 0.6548 - loss: 1.0201 - val_accuracy: 0.6688 - val_loss: 1.0182\n",
      "Epoch 3/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 44ms/step - accuracy: 0.7182 - loss: 0.8420 - val_accuracy: 0.6817 - val_loss: 0.9843\n",
      "Epoch 4/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 45ms/step - accuracy: 0.7442 - loss: 0.7699 - val_accuracy: 0.6949 - val_loss: 0.9694\n",
      "Epoch 5/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 45ms/step - accuracy: 0.7575 - loss: 0.7294 - val_accuracy: 0.7012 - val_loss: 0.9519\n",
      "Epoch 6/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 45ms/step - accuracy: 0.7622 - loss: 0.7135 - val_accuracy: 0.6985 - val_loss: 0.9670\n",
      "Epoch 7/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 44ms/step - accuracy: 0.7691 - loss: 0.6940 - val_accuracy: 0.7024 - val_loss: 0.9692\n",
      "Epoch 8/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 45ms/step - accuracy: 0.7736 - loss: 0.6777 - val_accuracy: 0.7083 - val_loss: 0.9535\n",
      "Epoch 9/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 51ms/step - accuracy: 0.7765 - loss: 0.6693 - val_accuracy: 0.7070 - val_loss: 0.9574\n",
      "Epoch 10/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 51ms/step - accuracy: 0.7792 - loss: 0.6599 - val_accuracy: 0.7101 - val_loss: 0.9592\n",
      "Epoch 11/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 49ms/step - accuracy: 0.7822 - loss: 0.6518 - val_accuracy: 0.7113 - val_loss: 0.9676\n",
      "Epoch 12/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 46ms/step - accuracy: 0.7842 - loss: 0.6456 - val_accuracy: 0.7095 - val_loss: 0.9616\n",
      "Epoch 13/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 52ms/step - accuracy: 0.7858 - loss: 0.6377 - val_accuracy: 0.7115 - val_loss: 0.9652\n",
      "Epoch 14/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 51ms/step - accuracy: 0.7874 - loss: 0.6324 - val_accuracy: 0.7087 - val_loss: 0.9794\n",
      "Epoch 15/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 46ms/step - accuracy: 0.7881 - loss: 0.6299 - val_accuracy: 0.7142 - val_loss: 0.9882\n",
      "Epoch 16/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 45ms/step - accuracy: 0.7904 - loss: 0.6230 - val_accuracy: 0.7148 - val_loss: 0.9856\n",
      "Epoch 17/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 46ms/step - accuracy: 0.7932 - loss: 0.6168 - val_accuracy: 0.7112 - val_loss: 0.9851\n",
      "Epoch 18/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 47ms/step - accuracy: 0.7933 - loss: 0.6143 - val_accuracy: 0.7137 - val_loss: 0.9727\n",
      "Epoch 19/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 46ms/step - accuracy: 0.7956 - loss: 0.6081 - val_accuracy: 0.7139 - val_loss: 0.9824\n",
      "Epoch 20/20\n",
      "\u001b[1m6919/6919\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 45ms/step - accuracy: 0.7964 - loss: 0.6062 - val_accuracy: 0.7071 - val_loss: 0.9871\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_cat,\n",
    "    validation_data=(X_val_scaled, y_val_cat),\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    verbose=1,\n",
    "    # callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48357699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7026 | Test loss: 0.9359\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test_scaled, y_test_cat, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f} | Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2fd56f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2043/2043\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 16ms/step\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                  bicycling       0.33      0.36      0.34       713\n",
      "             brushing teeth       0.79      0.71      0.75       434\n",
      "                   clapping       0.96      0.89      0.92       433\n",
      "    dribbling (basket ball)       0.78      0.96      0.86       441\n",
      "                   drinking       0.71      0.71      0.71       448\n",
      "               eating chips       0.55      0.71      0.62       437\n",
      "               eating pasta       0.69      0.73      0.71       443\n",
      "            eating sandwich       0.48      0.37      0.42       436\n",
      "                eating soup       0.78      0.82      0.80       431\n",
      "            folding clothes       0.87      0.90      0.88       370\n",
      "           household-chores       0.53      0.55      0.54      5596\n",
      "                    jogging       0.88      0.99      0.93       449\n",
      "      kicking (soccer ball)       0.59      0.75      0.66       433\n",
      "                manual-work       0.30      0.28      0.29       680\n",
      "             mixed-activity       0.29      0.07      0.11      3120\n",
      "playing catch (tennis ball)       0.91      0.76      0.83       431\n",
      "                    sitting       0.67      0.79      0.73     17104\n",
      "                      sleep       0.92      0.93      0.92     21241\n",
      "                     sports       0.66      0.07      0.13       835\n",
      "                     stairs       0.76      0.48      0.59       424\n",
      "                   standing       0.51      0.18      0.27      2563\n",
      "                     typing       0.78      0.71      0.74       430\n",
      "                    vehicle       0.45      0.30      0.36      1836\n",
      "                    walking       0.42      0.55      0.47      5165\n",
      "                    writing       0.74      0.85      0.79       453\n",
      "\n",
      "                   accuracy                           0.70     65346\n",
      "                  macro avg       0.65      0.62      0.62     65346\n",
      "               weighted avg       0.69      0.70      0.68     65346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_labels = le.inverse_transform(np.argmax(y_pred, axis=1))\n",
    "y_true_labels = le.inverse_transform(np.argmax(y_test_cat, axis=1))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true_labels, y_pred_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
