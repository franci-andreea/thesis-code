{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62eacf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d702994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 21:03:35.550608: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750097015.568596   19843 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750097015.574358   19843 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750097015.588822   19843 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750097015.588845   19843 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750097015.588848   19843 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750097015.588851   19843 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-16 21:03:35.593480: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Bidirectional, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "940673dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7576839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets\n",
    "train_df = pd.read_csv('./train_data.csv')\n",
    "validation_df = pd.read_csv('./validation_data.csv')\n",
    "test_df = pd.read_csv('./test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66017f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON_ID                            130\n",
      "segment_id                        100900\n",
      "ACTIVITY                     eating soup\n",
      "start_time    2021-01-01 02:03:00.450000\n",
      "end_time      2021-01-01 02:03:00.650000\n",
      "row_count                              2\n",
      "duration                             0.2\n",
      "Name: 100899, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Ensure timestamp is datetime and sorted\n",
    "train_df_copy = train_df\n",
    "# train_df_copy['TIMESTAMP'] = pd.to_datetime(train_df_copy['TIMESTAMP'])\n",
    "train_df_copy = train_df_copy.sort_values(['PERSON_ID', 'TIMESTAMP'])\n",
    "\n",
    "# Detect changes in person or activity\n",
    "train_df_copy['activity_change'] = (\n",
    "    (train_df_copy['ACTIVITY'] != train_df_copy['ACTIVITY'].shift()) |\n",
    "    (train_df_copy['PERSON_ID'] != train_df_copy['PERSON_ID'].shift())\n",
    ")\n",
    "\n",
    "# Assign a unique segment ID to each continuous activity\n",
    "train_df_copy['segment_id'] = train_df_copy['activity_change'].cumsum()\n",
    "\n",
    "# Get start and end time of each segment\n",
    "segment_info = train_df_copy.groupby(['PERSON_ID', 'segment_id', 'ACTIVITY']).agg(\n",
    "    start_time=('TIMESTAMP', 'first'),\n",
    "    end_time=('TIMESTAMP', 'last'),\n",
    "    row_count=('TIMESTAMP', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Ensure datetime types and calculate duration\n",
    "segment_info['start_time'] = pd.to_datetime(segment_info['start_time'])\n",
    "segment_info['end_time'] = pd.to_datetime(segment_info['end_time'])\n",
    "segment_info['duration'] = (segment_info['end_time'] - segment_info['start_time']).dt.total_seconds()\n",
    "\n",
    "# Filter out 0 or negative durations\n",
    "segment_info = segment_info[segment_info['duration'] > 0]\n",
    "\n",
    "# Find the row with the minimum duration\n",
    "shortest_segment = segment_info.loc[segment_info['duration'].idxmin()]\n",
    "print(shortest_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabb1bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        segment_id  PERSON_ID              start_time                end_time  \\\n",
      "100899      100900        130 2021-01-01 02:03:00.450 2021-01-01 02:03:00.650   \n",
      "\n",
      "        row_count  duration  \n",
      "100899          2       0.2  \n"
     ]
    }
   ],
   "source": [
    "# Get all eating soup segments\n",
    "soup_segments = segment_info[segment_info['ACTIVITY'] == 'eating soup']\n",
    "\n",
    "# Sort by duration to see shortest/longest\n",
    "soup_segments = soup_segments.sort_values(by='duration')\n",
    "print(soup_segments[['segment_id', 'PERSON_ID', 'start_time', 'end_time', 'row_count', 'duration']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "367405bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD SLIDING WINDOW\n",
    "# df - dataframe used\n",
    "# window_size - size of the sliding window, by default 11s if not mentioned otherwise\n",
    "# step_size - starting point for the current window given the previous, by default 5\n",
    "# feature_cols - features to be used in the sliding window\n",
    "def create_windows(dataset, window_size=11, step_size=5, feature_cols=['ACC_X', 'ACC_Y', 'ACC_Z']):\n",
    "    X = []\n",
    "    y = []\n",
    "    window = []\n",
    "\n",
    "    for person_id in dataset['PERSON_ID'].unique():\n",
    "        person_data = dataset[dataset['PERSON_ID'] == person_id]\n",
    "        feature_values = person_data[feature_cols].values\n",
    "        activity = person_data['ACTIVITY']\n",
    "\n",
    "        max_window_end = len(person_data)\n",
    "\n",
    "        for i in range(0, max_window_end - window_size, step_size):\n",
    "            window = feature_values[i:i+window_size]\n",
    "            window_label = activity[i:i+window_size].mode(dropna=False).iloc[0]\n",
    "\n",
    "            # Ensure the window is of the correct size\n",
    "            if len(window) != window_size:\n",
    "                continue  # Skip this window if it's the wrong shape\n",
    "\n",
    "            X.append(window)\n",
    "            y.append(window_label)\n",
    "\n",
    "    print(len(X))\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddfb7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 180\n",
    "step_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f4637b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590010\n",
      "166141\n",
      "85725\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = create_windows(train_df, window_size, step_size)\n",
    "X_val, y_val = create_windows(validation_df, window_size, step_size)\n",
    "X_test, y_test = create_windows(test_df, window_size, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f06d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of windows per class to see the imbalance ratio among windows\n",
    "def print_window_distribution(y_labels):\n",
    "    class_counts = Counter(y_labels)\n",
    "    sorted_counts = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"Window count per class (descending):\")\n",
    "    for label, count in sorted_counts:\n",
    "        print(f\"{label:20} {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9183b100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window count per class (descending):\n",
      "sleep                309925\n",
      "sitting              162784\n",
      "household-chores     28358\n",
      "walking              26217\n",
      "vehicle              16155\n",
      "mixed-activity       15977\n",
      "standing             14112\n",
      "bicycling            4772\n",
      "manual-work          3549\n",
      "sports               1808\n",
      "writing              453\n",
      "jogging              449\n",
      "drinking             448\n",
      "eating pasta         443\n",
      "dribbling (basket ball) 441\n",
      "eating chips         437\n",
      "eating sandwich      436\n",
      "brushing teeth       434\n",
      "kicking (soccer ball) 433\n",
      "clapping             433\n",
      "eating soup          431\n",
      "playing catch (tennis ball) 431\n",
      "typing               430\n",
      "stairs               424\n",
      "folding clothes      230\n"
     ]
    }
   ],
   "source": [
    "print_window_distribution(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a37455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_upweight_majority_class(X_train, y_train, downsample_factor, majority_class):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    majority_class_indices = np.where(y_train == majority_class)[0]\n",
    "\n",
    "    X_majority_class = X_train[majority_class_indices]\n",
    "    y_majority_class = y_train[majority_class_indices]\n",
    "\n",
    "    print(f\"total number of rows on X for majority class {majority_class}: {len(X_majority_class)}\")\n",
    "    print(f\"total number of rows on y for majority class {majority_class}: {len(y_majority_class)}\")\n",
    "\n",
    "    number_of_majority_samples = len(X_majority_class)\n",
    "    number_of_samples_to_extract = number_of_majority_samples // downsample_factor\n",
    "\n",
    "    random_chosen_indices = np.random.choice(number_of_majority_samples, number_of_samples_to_extract, replace=False)\n",
    "\n",
    "    # downsampled_X = X_majority_class[random_chosen_indices]\n",
    "    # downsampled_y = y_majority_class[random_chosen_indices]\n",
    "\n",
    "    new_X_train = []\n",
    "    new_y_train = []\n",
    "    new_sample_weights = []\n",
    "\n",
    "    selected_majority_indices = majority_class_indices[random_chosen_indices]\n",
    "\n",
    "    for index in range(0, len(X_train)):\n",
    "        if index in selected_majority_indices:\n",
    "            new_X_train.append(X_train[index])\n",
    "            # new_sample_weights.append(sample_weights[index] * downsample_factor)\n",
    "            new_y_train.append(y_train[index])\n",
    "        elif index in majority_class_indices:\n",
    "            continue\n",
    "        else:\n",
    "            new_X_train.append(X_train[index])\n",
    "            # new_sample_weights.append(sample_weights[index])\n",
    "            new_y_train.append(y_train[index])\n",
    "    \n",
    "    return np.array(new_X_train), np.array(new_y_train)#, np.array(new_sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a397409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = np.full(len(X_train), 1) # initialize weights array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39194c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_downsample_factors = {\n",
    "    'sleep': 12,\n",
    "    'sitting': 6,\n",
    "    # 'household-chores': 2,\n",
    "    # 'walking': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acf694c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of rows on X for majority class sleep: 309925\n",
      "total number of rows on y for majority class sleep: 309925\n",
      "total number of rows on X for majority class sitting: 162784\n",
      "total number of rows on y for majority class sitting: 162784\n"
     ]
    }
   ],
   "source": [
    "for class_name, downsample_factor in class_downsample_factors.items():\n",
    "    new_X_train, new_y_train = downsample_upweight_majority_class(X_train, y_train, downsample_factor, class_name)\n",
    "    X_train = new_X_train\n",
    "    y_train = new_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "991afc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = new_X_train\n",
    "y_train = new_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3de6becd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window count per class (descending):\n",
      "household-chores     28358\n",
      "sitting              27130\n",
      "walking              26217\n",
      "sleep                25827\n",
      "vehicle              16155\n",
      "mixed-activity       15977\n",
      "standing             14112\n",
      "bicycling            4772\n",
      "manual-work          3549\n",
      "sports               1808\n",
      "writing              453\n",
      "jogging              449\n",
      "drinking             448\n",
      "eating pasta         443\n",
      "dribbling (basket ball) 441\n",
      "eating chips         437\n",
      "eating sandwich      436\n",
      "brushing teeth       434\n",
      "kicking (soccer ball) 433\n",
      "clapping             433\n",
      "eating soup          431\n",
      "playing catch (tennis ball) 431\n",
      "typing               430\n",
      "stairs               424\n",
      "folding clothes      230\n"
     ]
    }
   ],
   "source": [
    "print_window_distribution(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73ed865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE DATA\n",
    "scaler = StandardScaler()\n",
    "\n",
    "n_samples = X_train.shape[0]\n",
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83ffe7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flat = X_train.reshape(-1, X_train.shape[-1])  # Flatten each window into a 1D array\n",
    "X_val_flat = X_val.reshape(-1, X_val.shape[-1])\n",
    "X_test_flat = X_test.reshape(-1, X_test.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79ea5833",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train_flat).reshape(n_samples, n_timesteps, n_features)\n",
    "X_val_scaled = scaler.transform(X_val_flat).reshape(X_val.shape[0], n_timesteps, n_features)\n",
    "X_test_scaled = scaler.transform(X_test_flat).reshape(X_test.shape[0], n_timesteps, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "007f51fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smote_input = X_train_scaled.reshape((X_train_scaled.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0b834d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_smote_input, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be30ca89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window count per class (descending):\n",
      "sleep                28358\n",
      "walking              28358\n",
      "jogging              28358\n",
      "stairs               28358\n",
      "sitting              28358\n",
      "standing             28358\n",
      "typing               28358\n",
      "brushing teeth       28358\n",
      "eating soup          28358\n",
      "eating chips         28358\n",
      "eating pasta         28358\n",
      "drinking             28358\n",
      "eating sandwich      28358\n",
      "kicking (soccer ball) 28358\n",
      "playing catch (tennis ball) 28358\n",
      "dribbling (basket ball) 28358\n",
      "writing              28358\n",
      "clapping             28358\n",
      "folding clothes      28358\n",
      "household-chores     28358\n",
      "vehicle              28358\n",
      "mixed-activity       28358\n",
      "bicycling            28358\n",
      "sports               28358\n",
      "manual-work          28358\n"
     ]
    }
   ],
   "source": [
    "print_window_distribution(y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a4eabf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled_ts = X_train_resampled.reshape((-1, n_timesteps, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fa06458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODE LABELS\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train_resampled)\n",
    "y_val_enc = le.transform(y_val)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_cat = to_categorical(y_train_enc)\n",
    "y_val_cat = to_categorical(y_val_enc)\n",
    "y_test_cat = to_categorical(y_test_enc)\n",
    "\n",
    "num_classes = y_train_cat.shape[1]  # Number of unique classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bcef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the LSTM model\n",
    "# model = Sequential([\n",
    "#     LSTM(128, input_shape=(X_resampled_ts.shape[1], X_resampled_ts.shape[2]), return_sequences=False),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(num_classes, activation='softmax')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5e5b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential([\n",
    "#     LSTM(128, input_shape=(X_resampled_ts.shape[1], X_resampled_ts.shape[2]), return_sequences=True),\n",
    "#     Dropout(0.3),\n",
    "#     LSTM(64, return_sequences=False), # Returns only the last output\n",
    "#     Dropout(0.5),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(num_classes, activation='softmax')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7e1313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential([\n",
    "#     Conv1D(filters=64, kernel_size=3, activation='relu', padding='same',\n",
    "#            input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])),\n",
    "#     Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#     Dropout(0.3),\n",
    "#     LSTM(128, return_sequences=False),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(num_classes, activation='softmax')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential([\n",
    "#     Conv1D(filters=64, kernel_size=3, activation='relu', padding='same',\n",
    "#            input_shape=(X_resampled_ts.shape[1], X_resampled_ts.shape[2])),\n",
    "#     Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#     Dropout(0.3),\n",
    "#     Bidirectional(LSTM(128, return_sequences=False)),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(num_classes, activation='softmax')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12378100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential([\n",
    "#     Conv1D(filters=64, kernel_size=3, activation='relu', padding='same',\n",
    "#            input_shape=(X_resampled_ts.shape[1], X_resampled_ts.shape[2])),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#     Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#     Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#     Dropout(0.3),\n",
    "#     Bidirectional(LSTM(128, return_sequences=False)),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(num_classes, activation='softmax')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential([\n",
    "#     Conv1D(filters=64, kernel_size=3, activation='relu', padding='same',\n",
    "#            input_shape=(X_resampled_ts.shape[1], X_resampled_ts.shape[2])),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#     Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#     Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#     Dropout(0.3),\n",
    "#     Bidirectional(GRU(128, return_sequences=False)),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(num_classes, activation='softmax')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ec28c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    GRU(128, return_sequences=True, input_shape=(X_resampled_ts.shape[1], X_resampled_ts.shape[2])),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    GRU(64),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42468161",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6164288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3d86992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 22:47:48.365987: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1531332000 exceeds 10% of free system memory.\n",
      "2025-06-16 22:47:49.586489: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1531332000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m11078/11078\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 32ms/step - accuracy: 0.4373 - loss: 1.6078 - val_accuracy: 0.3084 - val_loss: 2.2329\n",
      "Epoch 2/100\n",
      "\u001b[1m11078/11078\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 32ms/step - accuracy: 0.7195 - loss: 0.7885 - val_accuracy: 0.6457 - val_loss: 1.0917\n",
      "Epoch 3/100\n",
      "\u001b[1m11078/11078\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 32ms/step - accuracy: 0.7735 - loss: 0.6507 - val_accuracy: 0.6785 - val_loss: 1.0473\n",
      "Epoch 4/100\n",
      "\u001b[1m11078/11078\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 32ms/step - accuracy: 0.7970 - loss: 0.5891 - val_accuracy: 0.7120 - val_loss: 1.0131\n",
      "Epoch 5/100\n",
      "\u001b[1m11078/11078\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 32ms/step - accuracy: 0.8182 - loss: 0.5380 - val_accuracy: 0.7375 - val_loss: 0.9497\n",
      "Epoch 6/100\n",
      "\u001b[1m11078/11078\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 32ms/step - accuracy: 0.8357 - loss: 0.4898 - val_accuracy: 0.6958 - val_loss: 1.0382\n",
      "Epoch 7/100\n",
      "\u001b[1m11078/11078\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 32ms/step - accuracy: 0.8466 - loss: 0.4599 - val_accuracy: 0.7274 - val_loss: 1.0144\n",
      "Epoch 8/100\n",
      "\u001b[1m11078/11078\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 33ms/step - accuracy: 0.8535 - loss: 0.4364 - val_accuracy: 0.7387 - val_loss: 0.9945\n",
      "Epoch 9/100\n",
      "\u001b[1m11078/11078\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 33ms/step - accuracy: 0.8582 - loss: 0.4223 - val_accuracy: 0.7378 - val_loss: 0.9916\n",
      "Epoch 10/100\n",
      "\u001b[1m11078/11078\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 33ms/step - accuracy: 0.8617 - loss: 0.4111 - val_accuracy: 0.7330 - val_loss: 0.9907\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(\n",
    "    X_resampled_ts, y_train_cat,\n",
    "    validation_data=(X_val_scaled, y_val_cat),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "460e6d81",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# evaluate\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m test_loss, test_acc = \u001b[43mmodel\u001b[49m.evaluate(X_test_scaled, y_test_cat, verbose=\u001b[32m0\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Test loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test_scaled, y_test_cat, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f} | Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358d3ae3",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: CUDA error: : CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m y_pred = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m y_pred_labels = le.inverse_transform(np.argmax(y_pred, axis=\u001b[32m1\u001b[39m))\n\u001b[32m      3\u001b[39m y_true_labels = le.inverse_transform(np.argmax(y_test_cat, axis=\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/licenta/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/licenta/.venv/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[39m, in \u001b[36mconvert_to_eager_tensor\u001b[39m\u001b[34m(value, ctx, dtype)\u001b[39m\n\u001b[32m    106\u001b[39m     dtype = dtypes.as_dtype(dtype).as_datatype_enum\n\u001b[32m    107\u001b[39m ctx.ensure_initialized()\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mInternalError\u001b[39m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: CUDA error: : CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_labels = le.inverse_transform(np.argmax(y_pred, axis=1))\n",
    "y_true_labels = le.inverse_transform(np.argmax(y_test_cat, axis=1))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true_labels, y_pred_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
