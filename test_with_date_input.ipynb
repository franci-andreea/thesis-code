{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2548f145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 16:30:49.465200: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750339849.563589    6250 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750339849.591882    6250 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750339849.794975    6250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750339849.795005    6250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750339849.795008    6250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750339849.795010    6250 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-19 16:30:49.820190: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Bidirectional, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a81cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def handle_empty_activity(activity):\n",
    "#     if pd.isna(activity):\n",
    "#         activity = 'NAN'\n",
    "#     return activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56585bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read capture24 dataset\n",
    "# fp = '/home/franci/licenta/datasets_archives/capture24/'\n",
    "# capture24_csv_files = glob.glob(fp + 'P*.csv')\n",
    "# capture24_csv_files.sort()\n",
    "\n",
    "# print(capture24_csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fdcfa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL_SET = 'label:WillettsSpecific2018'\n",
    "# activity_labels = 'annotation-label-dictionary.csv'\n",
    "# annotations_label_dict = pd.read_csv(fp + activity_labels, index_col='annotation', dtype='string')\n",
    "\n",
    "# person_id = 152\n",
    "# for f in capture24_csv_files:\n",
    "#     person_df = pd.read_csv(f, header=0, names=['TIMESTAMP', 'ACC_X', 'ACC_Y', 'ACC_Z', 'ANNOTATION'])\n",
    "#     person_df['ACTIVITY'] = annotations_label_dict[LABEL_SET].reindex(person_df['ANNOTATION']).to_numpy()\n",
    "#     person_df['ACTIVITY'] = person_df['ACTIVITY'].apply(lambda x: handle_empty_activity(x))\n",
    "#     person_df = person_df.drop('ANNOTATION', axis=1)\n",
    "#     person_df = person_df[person_df['ACTIVITY'] != 'NAN']\n",
    "#     person_df['PERSON_ID'] = person_id\n",
    "#     cols = person_df.columns.to_list()\n",
    "#     cols = cols[-1:] + cols[:-1]\n",
    "#     person_df = person_df[cols]\n",
    "#     person_df = person_df[::100]\n",
    "\n",
    "#     if person_id == 152:\n",
    "#         person_df.to_csv(f, mode='w+', index=False)\n",
    "#         print(\"Saved first file with header!\\n\")\n",
    "#     else:\n",
    "#         person_df.to_csv(f, mode='w+', index=False, header=False)\n",
    "\n",
    "#     print(person_id)\n",
    "#     person_id = person_id + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16061bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset & split \n",
    "capture24 = pd.read_csv('./all_capture24_original_timestamp.csv')\n",
    "capture24['TIMESTAMP'] = pd.to_datetime(capture24['TIMESTAMP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc29b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df):\n",
    "    train_df = df[(df['PERSON_ID'] >= 152) & (df['PERSON_ID'] <= 257)].copy()\n",
    "    val_df = df[(df['PERSON_ID'] >= 258) & (df['PERSON_ID'] <= 287)].copy()\n",
    "    test_df = df[(df['PERSON_ID'] >= 288) & (df['PERSON_ID'] <= 302)].copy()\n",
    "\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17b83541",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df, test_df = split_dataset(capture24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c50a3362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD SLIDING WINDOW\n",
    "# df - dataframe used\n",
    "# window_size - size of the sliding window, by default 11s if not mentioned otherwise\n",
    "# step_size - starting point for the current window given the previous, by default 5\n",
    "# feature_cols - features to be used in the sliding window\n",
    "def create_windows(dataset, window_size=11, step_size=5, feature_cols=['ACC_X', 'ACC_Y', 'ACC_Z']):\n",
    "    X = []\n",
    "    y = []\n",
    "    window = []\n",
    "\n",
    "    for person_id in dataset['PERSON_ID'].unique():\n",
    "        person_data = dataset[dataset['PERSON_ID'] == person_id]\n",
    "        feature_values = person_data[feature_cols].values\n",
    "        activity = person_data['ACTIVITY']\n",
    "\n",
    "        max_window_end = len(person_data)\n",
    "\n",
    "        for i in range(0, max_window_end - window_size, step_size):\n",
    "            window = feature_values[i:i+window_size]\n",
    "            window_label = activity[i:i+window_size].mode(dropna=False).iloc[0]\n",
    "\n",
    "            # Ensure the window is of the correct size\n",
    "            if len(window) != window_size:\n",
    "                continue  # Skip this window if it's the wrong shape\n",
    "\n",
    "            X.append(window)\n",
    "            y.append(window_label)\n",
    "\n",
    "    print(len(X))\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2060017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 180\n",
    "step_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fb1b7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430280\n",
      "121109\n",
      "57454\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = create_windows(train_df, window_size, step_size)\n",
    "X_val, y_val = create_windows(validation_df, window_size, step_size)\n",
    "X_test, y_test = create_windows(test_df, window_size, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "630c3632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430280\n",
      "121109\n",
      "57454\n"
     ]
    }
   ],
   "source": [
    "X_time_train, y_time_train = create_windows(train_df, window_size, step_size, ['TIMESTAMP'])\n",
    "X_time_val, y_time_val = create_windows(validation_df, window_size, step_size, ['TIMESTAMP'])\n",
    "X_time_test, y_time_test = create_windows(test_df, window_size, step_size, ['TIMESTAMP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87afaafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of windows per class to see the imbalance ratio among windows\n",
    "def print_window_distribution(y_labels):\n",
    "    class_counts = Counter(y_labels)\n",
    "    sorted_counts = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"Window count per class (descending):\")\n",
    "    for label, count in sorted_counts:\n",
    "        print(f\"{label:20} {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "862159bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window count per class (descending):\n",
      "sleep                160374\n",
      "sitting              160205\n",
      "household-chores     28000\n",
      "walking              25919\n",
      "vehicle              16083\n",
      "mixed-activity       15977\n",
      "standing             13593\n",
      "bicycling            4772\n",
      "manual-work          3549\n",
      "sports               1808\n"
     ]
    }
   ],
   "source": [
    "print_window_distribution(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a25b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE DATA\n",
    "scaler = StandardScaler()\n",
    "\n",
    "n_samples = X_train.shape[0]\n",
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc36c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flat = X_train.reshape(-1, X_train.shape[-1])  # Flatten each window into a 1D array\n",
    "X_val_flat = X_val.reshape(-1, X_val.shape[-1])\n",
    "X_test_flat = X_test.reshape(-1, X_test.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9614f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train_flat).reshape(n_samples, n_timesteps, n_features)\n",
    "X_val_scaled = scaler.transform(X_val_flat).reshape(X_val.shape[0], n_timesteps, n_features)\n",
    "X_test_scaled = scaler.transform(X_test_flat).reshape(X_test.shape[0], n_timesteps, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9984c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODE LABELS\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_val_enc = le.transform(y_val)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_cat = to_categorical(y_train_enc)\n",
    "y_val_cat = to_categorical(y_val_enc)\n",
    "y_test_cat = to_categorical(y_test_enc)\n",
    "\n",
    "num_classes = y_train_cat.shape[1]  # Number of unique classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d01a6b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750339959.009410    6250 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2069 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Main sequence input\n",
    "sequence_input = Input(shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]))\n",
    "x = GRU(128, return_sequences=True)(sequence_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = GRU(64)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Second input: time features\n",
    "time_input = Input(shape=(X_time_train.shape[1],))  # flat input\n",
    "\n",
    "# Combine both\n",
    "combined = Concatenate()([x, time_input])\n",
    "\n",
    "# Dense layers\n",
    "x = Dense(64, activation='relu')(combined)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Define the model with two inputs\n",
    "model = Model(inputs=[sequence_input, time_input], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92f0de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f84833bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78d74025",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid dtype: datetime64[ns]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_time_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train_cat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_val_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_time_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_cat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/licenta/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/licenta/.venv/lib/python3.12/site-packages/optree/ops.py:766\u001b[39m, in \u001b[36mtree_map\u001b[39m\u001b[34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[39m\n\u001b[32m    764\u001b[39m leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[32m    765\u001b[39m flat_args = [leaves] + [treespec.flatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[43m.\u001b[49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: Invalid dtype: datetime64[ns]"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.fit(\n",
    "    [X_train_scaled, X_time_train],\n",
    "    y_train_cat,\n",
    "    validation_data=([X_val_scaled, X_time_val], y_val_cat), \n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c6f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "test_loss, test_acc = model.evaluate([X_test_scaled, X_time_test], y_test_cat, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f} | Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4da8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_labels = le.inverse_transform(np.argmax(y_pred, axis=1))\n",
    "y_true_labels = le.inverse_transform(np.argmax(y_test_cat, axis=1))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true_labels, y_pred_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
